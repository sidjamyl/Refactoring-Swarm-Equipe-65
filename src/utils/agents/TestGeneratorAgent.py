from src.utils.state.stateDefinition import SwarmState
from src.utils.prompts.promptTestGenerator import SYSTEM_PROMPT_TEST_GENERATOR, USER_PROMPT_TEST_GENERATOR, USER_PROMPT_TEST_GENERATOR_APPEND
import src.utils.agents.agentTest as agentTest
from pathlib import Path
import os
from src.utils.logger import log_experiment, ActionType


def test_generator_agent_node(state: SwarmState) -> dict:
    """G√©n√®re des tests unitaires pour les fonctions sans tests."""
    current_file = Path(state["current_file"]).resolve()
    refactored_code = state.get("refactored_code", "")
    functions_without_tests = state.get("function_without_tests", [])
    target_dir = Path(state["target_dir"]).resolve()
    
    print(f"\n{'‚îÄ'*80}")
    print(f"üî¨ G√âN√âRATION DE TESTS UNITAIRES")
    print(f"{'‚îÄ'*80}")
    print(f"\nüìã Fonctions √† tester : {', '.join(functions_without_tests) if functions_without_tests else 'Aucune'}")
    
    # Forcer la g√©n√©ration m√™me si la liste est vide (pour debug)
    if not functions_without_tests:
        print("‚ö†Ô∏è  Aucune fonction d√©tect√©e, g√©n√©ration forc√©e")
        functions_without_tests = ["all_functions"]
    
    # Construire le prompt
    test_file = (target_dir / "tests" / f"test_{current_file.stem}.py")
    existing_tests_content = ""
    is_append_mode = test_file.exists() and test_file.stat().st_size > 0

    # Calculer le module_name comme chemin relatif avec des points (ex: services.validators)
    module_name = current_file.relative_to(target_dir).with_suffix('').as_posix().replace('/', '.')

    if is_append_mode:
        try:
            existing_tests_content = test_file.read_text(encoding="utf-8")
            print(f"üìé Mode AJOUT : {test_file.name} existe d√©j√† ({len(existing_tests_content)} chars)")
        except Exception:
            is_append_mode = False

    if is_append_mode:
        user_prompt = USER_PROMPT_TEST_GENERATOR_APPEND.format(
            file_name=current_file.name,
            code=refactored_code if refactored_code else "# No code provided",
            functions_without_tests=", ".join(functions_without_tests),
            existing_tests=existing_tests_content,
            module_name=module_name
        )
    else:
        user_prompt = USER_PROMPT_TEST_GENERATOR.format(
            file_name=current_file.name,
            code=refactored_code if refactored_code else "# No code provided",
            functions_without_tests=", ".join(functions_without_tests),
            module_name=module_name
        )

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_TEST_GENERATOR},
        {"role": "user", "content": user_prompt}
    ]
    
    print(f"\nü§ñ Appel du LLM pour g√©n√©rer les tests...")
    
    # Appeler le LLM
    try:
        response = agentTest.model.invoke(messages)
        generated_tests = response.content
        status = "SUCCESS"
        print(f"‚úÖ Tests g√©n√©r√©s ({len(generated_tests)} caract√®res)")
    except Exception as e:
        print(f"‚ùå [TestGenerator] Erreur LLM: {e}")
        generated_tests = f"Error generating tests: {str(e)}"
        status = "FAILURE"
        
        # Logging de l'erreur
        log_experiment(
            agent_name="TestGenerator",
            model_used=agentTest.model.model,
            action=ActionType.GENERATION,
            details={
                "input_prompt": SYSTEM_PROMPT_TEST_GENERATOR + "\n" + user_prompt,
                "output_response": generated_tests
            },
            status=status
        )
        return {"status": "LLM_ERROR"}
    
    # Nettoyer le code g√©n√©r√©
    if "```python" in generated_tests:
        generated_tests = generated_tests.split("```python")[1].split("```")[0].strip()
    elif "```" in generated_tests:
        generated_tests = generated_tests.split("```")[1].split("```")[0].strip()
    
    # Cr√©er le dossier tests/ dans target_dir (√† c√¥t√© des fichiers source)
    tests_dir = target_dir / "tests"
    
    try:
        tests_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        print(f"‚ùå [TestGenerator] Erreur cr√©ation dossier: {e}")
        return {"status": "FOLDER_CREATION_ERROR"}
    
    # Cr√©er __init__.py pour que ce soit un package Python
    init_file = tests_dir / "__init__.py"
    if not init_file.exists():
        init_file.write_text("", encoding="utf-8")
    
    # Cr√©er le fichier de test
    test_file = tests_dir / f"test_{current_file.stem}.py"
    print(f"\nüíæ {'Ajout au' if is_append_mode else 'Cr√©ation du'} fichier de test : {test_file.name}")
    
    try:
        if is_append_mode:
            # S√©parer les imports des tests dans le code g√©n√©r√©
            new_lines = generated_tests.splitlines()
            new_import_lines = []
            new_test_lines = []
            for line in new_lines:
                stripped = line.strip()
                # Ne garder que les imports au niveau top-level (pas indent√©s)
                if (stripped.startswith("import ") or stripped.startswith("from ")) and not line[0:1].isspace():
                    new_import_lines.append(stripped)  # Toujours stocker la version sans indentation
                else:
                    new_test_lines.append(line)
            
            # Retirer les lignes vides en t√™te des tests
            while new_test_lines and not new_test_lines[0].strip():
                new_test_lines.pop(0)
            
            # Fusionner les imports manquants dans le fichier existant
            if new_import_lines:
                # D√©dupliquer les imports entre eux aussi
                seen = set()
                unique_imports = []
                for imp in new_import_lines:
                    if imp not in seen:
                        seen.add(imp)
                        unique_imports.append(imp)
                missing_imports = [
                    imp for imp in unique_imports
                    if imp and imp not in existing_tests_content
                ]
                if missing_imports:
                    # Ins√©rer apr√®s le dernier import du fichier existant
                    existing_lines = existing_tests_content.splitlines()
                    last_import_idx = 0
                    for i, line in enumerate(existing_lines):
                        stripped = line.strip()
                        if stripped.startswith("import ") or stripped.startswith("from "):
                            last_import_idx = i
                    # Ins√©rer les nouveaux imports juste apr√®s le dernier import existant
                    insert_at = last_import_idx + 1
                    existing_lines[insert_at:insert_at] = missing_imports
                    with open(test_file, "w", encoding="utf-8") as f:
                        f.write("\n".join(existing_lines) + "\n")
                    print(f"üì• {len(missing_imports)} import(s) ajout√©(s) : {[i.strip() for i in missing_imports]}")
            
            # Ajouter les nouveaux tests √† la fin
            test_code_to_append = "\n".join(new_test_lines).strip()
            if test_code_to_append:
                with open(test_file, "a", encoding="utf-8") as f:
                    f.write("\n\n\n# --- Tests g√©n√©r√©s automatiquement ---\n")
                    f.write(test_code_to_append)
                    f.write("\n")
            print(f"‚úÖ Tests ajout√©s avec succ√®s")
        else:
            # Cr√©er un nouveau fichier
            with open(test_file, "w", encoding="utf-8") as f:
                f.write(generated_tests)
        
        # V√©rifier que le fichier existe et a du contenu
        if test_file.exists():
            file_size = os.path.getsize(test_file)
            print(f"‚úÖ Fichier cr√©√© avec succ√®s ({file_size} bytes)")
            write_status = "SUCCESS"
        else:
            print(f"‚ùå √âchec de cr√©ation du fichier")
            write_status = "FAILURE"
        
        # Logging de l'exp√©rience
        log_experiment(
            agent_name="TestGenerator",
            model_used=agentTest.model.model,
            action=ActionType.GENERATION,
            details={
                "input_prompt": SYSTEM_PROMPT_TEST_GENERATOR + "\n" + user_prompt,
                "output_response": generated_tests
            },
            status=write_status
        )
        
        return {
            "status": "TESTS_GENERATED",
            "function_without_tests": [],
        }
    except Exception as e:
        print(f"‚ùå [TestGenerator] Erreur √©criture fichier: {e}")
        import traceback
        traceback.print_exc()
        return {
            "status": "TEST_GENERATION_FAILED",
            "function_without_tests": functions_without_tests
        }